{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  JELSR feature selection\n",
    "# reference :\"Joint Embedding Learning and Sparse Regression: A Framework for Unsupervised Feature Selection\"(2014)\n",
    "# by  Chenping Hou, Feiping Nie, Xuelong Li, Dongyun Yi and Yi Wu\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize, NonlinearConstraint\n",
    "from itertools import repeat\n",
    "\n",
    "\n",
    "# First define a r,p norm function. ||Q||_{r,p} defined in equation (9) in the paper. \n",
    "\n",
    "def LRPNorm(w,r=2,p=2):\n",
    "    \"\"\"\n",
    "    define L_{r,p} norm function to compute the norm with general r, p\n",
    "    -----\n",
    "    input\n",
    "    -----\n",
    "    w:{numpy array}, shape(n_rows,n_col) \n",
    "       input matrix \n",
    "\n",
    "    r:{int}\n",
    "       parameter r in l_{r,p} norm\n",
    "\n",
    "    p:{int}\n",
    "       parameter p in l_{r,p} norm\n",
    "   \n",
    "    ------\n",
    "    output\n",
    "    ------\n",
    "    resNorm:{float}\n",
    "        l_{r,p} norm\n",
    "\n",
    "    \"\"\"         \n",
    "    Sum = 0\n",
    "    row,col = w.shape\n",
    "    for i in range(row):\n",
    "        temp = 0\n",
    "        for j in range(col):\n",
    "            temp += np.power(np.absolute(w[i,j]),r) \n",
    "        temp = np.power(temp,p/r)\n",
    "        Sum += temp\n",
    "        resNorm = np.power(Sum, 1/p)\n",
    "    return resNorm\n",
    "\n",
    "# Notations: alpha , beta are balanced parameter. \n",
    "#            k: neighborhood size\n",
    "#            m: dimensionality of embedding\n",
    "#            s: selected feature number\n",
    "#            n: original data size\n",
    "#            d: dimensionality of original data\n",
    "\n",
    "def KNNgraphG(X,k):\n",
    "    \n",
    "    \"\"\"\n",
    "    Define K Nearest Neighbors Graph of matrix X.  \n",
    "    -----\n",
    "    input\n",
    "    -----\n",
    "    X: {numpy array},shape(n_features,n_samples)\n",
    "        input data\n",
    "\n",
    "    k: {int}\n",
    "        neighborhood size\n",
    "\n",
    "    ------\n",
    "    output\n",
    "    ------\n",
    "    GraphG:{dictionary}\n",
    "       return the index list of k nearest neighbors of each sample observation.\n",
    "\n",
    "    \"\"\"    \n",
    "    d, n = X.shape \n",
    "    if k > d:\n",
    "        k = d\n",
    "    Distance = np.array(list(repeat(0.0000,n**2))).reshape(n,n)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            Distance[i,j]= np.linalg.norm(X[:,i]-X[:,j])\n",
    "    GraphG = {}\n",
    "    for i in range(n):\n",
    "        GraphG[i] =list(np.argsort(Distance[i,])[0:k+1])\n",
    "        if i in GraphG[i]:\n",
    "            GraphG[i].remove(i)\n",
    "        GraphG[i] = sorted(GraphG[i])    \n",
    "    return GraphG\n",
    "\n",
    "\n",
    "def SimilarityM(X,k):\n",
    "    \n",
    "    \"\"\"\n",
    "    Define similarity matrix S.\n",
    "    -----\n",
    "    input\n",
    "    -----\n",
    "    X: {numpy array},shape(n_features,n_samples)\n",
    "        input data\n",
    "\n",
    "    k: {int}\n",
    "        neighborhood size\n",
    "    ------\n",
    "    output\n",
    "    ------\n",
    "    S: {numpy array},shape(n_samples,n_samples)\n",
    "       similarity matrix\n",
    "    \"\"\"    \n",
    "        \n",
    "    import statsmodels.api as sm\n",
    "    d,n = X.shape\n",
    "    GraphG = KNNgraphG(X,k)\n",
    "\n",
    "    S = np.array(list(repeat(0.0000,n**2))).reshape(n,n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        idx = GraphG[i]\n",
    "        regX = X[:,idx]\n",
    "        regy = X[:,i]\n",
    "        model = sm.OLS(regy,regX)\n",
    "        result = model.fit()\n",
    "        t = 0\n",
    "        for j in idx:\n",
    "            templist =  list(result.params/np.sum(result.params))\n",
    "            templist = [ round(a,4) for a in templist ]\n",
    "            S[i,j] = templist[t]\n",
    "            t += 1\n",
    "        \n",
    "    return S\n",
    "                \n",
    "\n",
    "def LaplaceM(X,k):\n",
    "\n",
    "    \"\"\"\n",
    "    Define Laplace Matrix \n",
    "    -----\n",
    "    input\n",
    "    -----\n",
    "    X: {numpy array},shape(n_features,n_samples)\n",
    "        input data\n",
    "\n",
    "    k: {int}\n",
    "        neighborhood size\n",
    "    ------\n",
    "    output\n",
    "    ------\n",
    "    L: {numpy array},shape(n_samples,n_samples)\n",
    "       Laplace matrix  \n",
    "\n",
    "    \"\"\"        \n",
    "    d,n = X.shape\n",
    "    temp = np.identity(n)- SimilarityM(X,k)\n",
    "    L = np.dot(temp.transpose() , temp)\n",
    "    return L\n",
    "\n",
    "\n",
    "def JELSR(X,m,k=3,maxiter= 500, r=2,p=1,alpha = 1, beta =1 , Tol = 1e-3):\n",
    "    \n",
    "    \"\"\"\n",
    "    JELSR function implement unsupervised feature selection using Joint Embedding Learning Sparse Regression analysis.\n",
    "    the optimization problem is the from equation (16) in the paper. \n",
    "    arg min_{W,YYT = I} tr(YLYT) + beta*(||WTX - Y ||_2 ^2 + alpha *||W||_{r,p}^p  ) \n",
    "\n",
    "    ------\n",
    "    Input\n",
    "    ------\n",
    "    X: {numpy array},shape(n_features,n_samples)\n",
    "        input data\n",
    "    m: {int}\n",
    "        dimensionality of embedding\n",
    "    k: {int}\n",
    "        neighborhood size,default value is 3 \n",
    "    maxiter:{int}\n",
    "        maximum number of iteration, default value is 500\n",
    "    r:{int}\n",
    "        parameter r in l_{r,p} norm, default value is 2\n",
    "    p:{int} \n",
    "        parameter p in l_{r,p} norm, default value is 1\n",
    "    alpha:{float}\n",
    "        parameter alpha in optimization problem, default value is 1\n",
    "    beta:{float}    \n",
    "        parameter beta in optimization problem, default value is 1\n",
    "    Tol:{float}    \n",
    "        tolerance to stop the optimization, default value is 1e-3\n",
    "    \n",
    "    -------\n",
    "    Output\n",
    "    -------\n",
    "  \n",
    "    W:{numpy array},shape(n_features,n_embeddings) \n",
    "      feature weight matrix\n",
    "    Y:{numpy array},shape(n_embeddings,n_samples)\n",
    "      policy matrix  \n",
    "\n",
    "    Reference:\n",
    "      \"Joint Embedding Learning and Sparse Regression: A Framework for Unsupervised Feature Selection\"(2014) \n",
    "       by  Chenping Hou, Feiping Nie, Xuelong Li, Dongyun Yi and Yi Wu\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    d,n = X.shape\n",
    "    U = np.identity(d)\n",
    "    L = LaplaceM(X,k)\n",
    "   \n",
    "    \n",
    "    # define objective function in equation (29) in the paper\n",
    "    def traceobj(y):\n",
    "        Y = np.array(y).reshape(m,n)\n",
    "        A = np.dot(X,X.transpose()) + alpha * U\n",
    "        Ainv = np.linalg.inv(A)\n",
    "        mid = LaplaceM(X,k) + beta * np.identity(n) - beta * np.dot(np.dot( X.transpose(), Ainv),X )\n",
    "        res1 = np.trace( np.dot(np.dot(Y,mid),Y.transpose() ))\n",
    "        return res1\n",
    "    \n",
    "    def constr_func(y):\n",
    "        Y = np.array(y).reshape(m,n)\n",
    "        res2 = np.linalg.norm( np.dot(Y,Y.transpose()) -np.identity(m) )\n",
    "        return res2\n",
    "   \n",
    "    y0 = np.zeros(m*n)\n",
    "    targetvalue = np.zeros(maxiter)\n",
    "    \n",
    "    for step in range(maxiter):\n",
    "        A = np.dot(X,X.transpose()) + alpha * U \n",
    "        nonlin_con = NonlinearConstraint(constr_func, lb = 0,ub =1e-4)  \n",
    "        res = minimize(traceobj,y0,constraints = nonlin_con, tol = 1e-4,options={'maxiter':500,'disp':False})\n",
    "        Y = res.x.reshape(m,n) \n",
    "        W = np.dot(np.dot(np.linalg.inv(A),X),Y.transpose())\n",
    "        ListW = []\n",
    "        for j in range(d):\n",
    "            ListW.append( 1/ (np.linalg.norm(W[j,])*2) )\n",
    "        U = np.diag(ListW)\n",
    "        y0 = Y.reshape(1,m*n)[0]\n",
    "        temp = np.linalg.norm( np.dot(W.transpose(),X)-Y )**2 + alpha * np.power(LRPNorm(W,r,p),p)\n",
    "        targetvalue[step] = np.trace(np.dot(np.dot(Y,L),Y.transpose())) + beta * temp\n",
    "        if step >= 1 and np.absolute(targetvalue[step]-targetvalue[step-1]) <= Tol:\n",
    "            break\n",
    "\n",
    "    return [W,Y]\n",
    "\n",
    "# next define the feature selection function, s is the desired number of features.\n",
    "\n",
    "def feature_selection(X,m,s,k=3):\n",
    "    \"\"\"\n",
    "    Define feature selection function\n",
    "    ------\n",
    "    Input\n",
    "    ------\n",
    "    X: {numpy array},shape(n_features,n_samples)\n",
    "        input data\n",
    "    m: {int}\n",
    "        dimensionality of embedding\n",
    "    s: {int}\n",
    "        number of features selected\n",
    "    k: {int}\n",
    "        neighborhood size,default value is 3 \n",
    "  \n",
    "    \"\"\"\n",
    "    d,n = X.shape\n",
    "    W = JELSR(X,k,m)[0]\n",
    "    scores = []\n",
    "    for i in range(d):\n",
    "        scores.append( np.linalg.norm(W[i,]))\n",
    "    selected = np.argsort(scores,0)[::-1][:s]    \n",
    "    return sorted(selected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [2, 3], 1: [0, 3], 2: [0, 3], 3: [0, 1]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([3,12,1,2,13,432,26,333,234,67,334,96,55,91,33,34]).reshape(4,4)\n",
    "KNNgraphG(x,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = JELSR(x,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selection(x,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
